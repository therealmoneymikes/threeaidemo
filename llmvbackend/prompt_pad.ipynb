{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLVM Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import csv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=os.getenv(\"RUNPOD_TOKEN\"), base_url=os.getenv('RUNPOND_CHATBOT_URL'))\n",
    "model_name = os.getenv('MODEL_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How old is Samuel L Jackson\"}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    top_p=0.8,\n",
    "    max_tokens=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_response(client, model_name, messages, temp=0):\n",
    "    prompt_messages = []\n",
    "    for message in prompt_messages:\n",
    "        prompt_messages.append({\"role\": message[\"role\"], \"content\": message[\"content\"]})\n",
    "        \n",
    "    response = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How old is Samuel L Jackson\"}\n",
    "    ],\n",
    "    temperature=temp,\n",
    "    top_p=0.8,\n",
    "    max_tokens=2000\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"How old is Samuel L Jackson\"}]\n",
    "response = get_chat_response(openai_client, model_name, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs (JSON) and CSV Files (CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "\n",
    "You are a helpful assistant that answers questions about  film stars.\n",
    "\n",
    "Your output should be in a structured json format. You are allowed to write data in a JSON object:\n",
    "[\n",
    "    {\n",
    "        \"icon\": \"The celebrity icon that you will get the information about\",\n",
    "        \"topic\": \"This information about the celebrity icon\"\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "messages.append({\"role\": \"user\", \"content\": \"How old is Samuel L Jackson\"})\n",
    "response = get_chat_response(openai_client, model_name, messages)\n",
    "\n",
    "\n",
    "try:\n",
    "    json_response = json.loads(response) # JSON Formatting\n",
    "    print(\"JSON Response:\", json_response)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "    \n",
    "csv_response = []\n",
    "\n",
    "if csv_response:\n",
    "    csv_filename = \"celebrity_info.csv\"\n",
    "    with open(csv_filename, mode=\"w\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        #header row\n",
    "        writer.writerow(['icon', 'topic'])\n",
    "        #data rows\n",
    "        writer.writerows(csv_response)\n",
    "        \n",
    "    print(f\"CSV File '{csv_filename}' has been successfully created.\")\n",
    "    \n",
    "else: \n",
    "    print(\"No Data received: CSV unable to be written \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Structuring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"\"\"\n",
    "    Get me info about famous film stars:\n",
    "    \n",
    "    ```\n",
    "        1. Samuel L Jackson\n",
    "        2. Chris Rock\n",
    "    ```\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "response = get_chat_response(openai_client, model_name, messages)\n",
    "\n",
    "json_response = json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give the model time to think\n",
    "# Giving the LLM more info makes the values more accurate\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "    Calculate the result of this question: 476*3/7000\n",
    "    \n",
    "    \n",
    "    Your outupt should in a structured json format, exactyl as the one below.\n",
    "    You cannot write anything other than the json object:\n",
    "    \n",
    "    {\n",
    "        steps: This is where you solve the equation set by set following the BIDMAS. You need to show your working out and calcuate each step leading to the final result.\n",
    "        result: The Final answer of the result of the equation above\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\":user_prompt}]\n",
    "response = get_chat_response(openai_client, model_name, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrival Augmented Generated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "What is newest features in a iphone 16\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "response = get_chat_response(openai_client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_16 = \"\"\"\n",
    "The iPhone 16 introduces several exciting updates, making it one of Apple's most advanced smartphones to date. It features a larger 6.1-inch display for the base model and a 6.7-inch screen for the iPhone 16 Plus, with thinner bezels and a more durable Ceramic Shield. The iPhone 16 Pro and Pro Max boast even larger displays, measuring 6.3 and 6.9 inches respectively, offering the thinnest bezels seen on any Apple product so far.\n",
    "\n",
    "Powered by the new A18 chip (A18 Pro for the Pro models), these phones deliver significant performance improvements, with enhanced neural engine capabilities, faster GPU for gaming, and machine learning tasks. The camera systems are also upgraded, with the base iPhone 16 sporting a dual-camera setup with a 48MP main sensor. The Pro models offer a 48MP Ultra Wide and 5x telephoto camera, enhanced by Appleâ€™s \"Camera Control\" button for more flexible photography options.\n",
    "\n",
    "Apple also introduced advanced audio features like \"Audio Mix,\" which uses machine learning to separate background sounds from speech, allowing for more refined audio capture during video recording. Battery life has been extended, especially in the iPhone 16 Pro Max, which is claimed to have the longest-lasting battery of any iPhone \n",
    "9TO5MAC\n",
    "\n",
    "APPLEMAGAZINE\n",
    ".\n",
    "\n",
    "Additionally, Apple has switched to USB-C for faster charging and data transfer, and the Pro models now support up to 2x faster video encoding. The starting prices remain consistent with previous generations, with the iPhone 16 starting at $799, while the Pro models start at $999\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\" \n",
    "{iphone_16}\n",
    "\n",
    "What's new in the iphone 16?\n",
    "\"\"\"\n",
    "\n",
    "messages = [{'role':'user','content':user_prompt}]\n",
    "response = get_chat_response(openai_client,model_name,messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
